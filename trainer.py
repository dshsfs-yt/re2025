from transformers import Seq2SeqTrainer
import torch
import torch.nn as nn
from typing import Optional, Dict, Union, Any, Tuple
from transformers.generation import LogitsProcessor, LogitsProcessorList
import torch

class RestrictVocabLogitsProcessor(LogitsProcessor):
    def __init__(self, allowed_token_ids):
        self.allowed_token_ids = torch.tensor(allowed_token_ids)

    def __call__(self, input_ids, scores):
        mask = torch.full_like(scores, float("-inf"))
        mask[:, self.allowed_token_ids] = scores[:, self.allowed_token_ids]
        return mask


class MonitoringSeq2SeqTrainer(Seq2SeqTrainer):
    """
    Seq2SeqTrainer with enhanced monitoring capabilities
    """
    
    def __init__(self, *args, restrict_decode_vocab=None, **kwargs):
        self.restrict_decode_vocab = restrict_decode_vocab
        super().__init__(*args, **kwargs)
        self.step_count = 0
        self.print_every_n_steps = 25  # ëª‡ ìŠ¤í…ë§ˆë‹¤ ì¶œë ¥í• ì§€
        
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        Override compute_loss to add monitoring
        """
        # ê¸°ë³¸ loss ê³„ì‚°
        loss, outputs = super().compute_loss(model, inputs, return_outputs=True)
        
        self.step_count += 1
        
        # ì£¼ê¸°ì ìœ¼ë¡œë§Œ ì¶œë ¥
        if self.step_count % self.print_every_n_steps == 0:
            self._print_model_outputs(inputs, outputs, loss)
        
        return (loss, outputs) if return_outputs else loss
    

    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):
        gen_kwargs = {
            "max_length": self.args.generation_max_length,
            "num_beams": self.args.generation_num_beams,
        }

        if self.restrict_decode_vocab is not None:
            processor = LogitsProcessorList(
                [RestrictVocabLogitsProcessor(self.restrict_decode_vocab)]
            )
            gen_kwargs["logits_processor"] = processor

        return super().prediction_step(
            model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs
        )

    def _print_model_outputs(self, inputs, outputs, loss):
        """ëª¨ë¸ ì¶œë ¥ì„ ê¹”ë”í•˜ê²Œ ì¶œë ¥"""
        
        print("\n" + "="*80)
        
        # Loss ì²˜ë¦¬ - í…ì„œì˜ ì°¨ì› í™•ì¸
        if isinstance(loss, torch.Tensor):
            if loss.dim() == 0:  # ìŠ¤ì¹¼ë¼
                loss_value = loss.item()
            else:  # ë²¡í„°ë‚˜ í–‰ë ¬
                loss_value = loss.mean().item()  # í‰ê· ê°’ ì‚¬ìš©
        else:
            loss_value = float(loss)
        
        print(f"[Step {self.step_count}] Loss: {loss_value:.4f}")
        print("="*80)
        
        # 1. ëª¨ë¸ì˜ logitsì—ì„œ ì˜ˆì¸¡ í† í° ì¶”ì¶œ (argmax)
        predictions = outputs.logits.argmax(dim=-1)
        
        # 2. ìž…ë ¥, ë ˆì´ë¸”, ì˜ˆì¸¡ ë””ì½”ë”©
        batch_size = min(3, predictions.shape[0])  # ìµœëŒ€ 3ê°œ ìƒ˜í”Œë§Œ ì¶œë ¥
        
        for i in range(batch_size):
            print(f"\n[Sample {i+1}]")
            
            # ìž…ë ¥ í…ìŠ¤íŠ¸
            if "input_ids" in inputs:
                input_text = self.tokenizer.decode(
                    inputs["input_ids"][i], 
                    skip_special_tokens=False
                )
                print(f"ðŸ“¥ Input: {input_text[:100]}{'...' if len(input_text) > 100 else ''}")
            
            # íƒ€ê²Ÿ (ë ˆì´ë¸”) í…ìŠ¤íŠ¸
            if "labels" in inputs:
                # -100ì„ pad_token_idë¡œ êµì²´
                labels = inputs["labels"][i]
                labels_clean = [
                    tok if tok != -100 else self.tokenizer.pad_token_id 
                    for tok in labels.tolist()
                ]
                target_text = self.tokenizer.decode(
                    labels_clean, 
                    skip_special_tokens=False
                )
                print(f"ðŸŽ¯ Target: {target_text[:100]}{'...' if len(target_text) > 100 else ''}")
            
            # ëª¨ë¸ ì˜ˆì¸¡ í…ìŠ¤íŠ¸
            pred_text = self.tokenizer.decode(
                predictions[i], 
                skip_special_tokens=False
            )
            print(f"ðŸ¤– Predicted: {pred_text[:100]}{'...' if len(pred_text) > 100 else ''}")
            
            # ê°„ë‹¨í•œ ì¼ì¹˜ ì—¬ë¶€ ì²´í¬
            if 'target_text' in locals():
                if pred_text.strip() == target_text.strip():
                    print(f"âœ… Exact match!")
                else:
                    # ì²˜ìŒ ëª‡ ê¸€ìžê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    min_len = min(len(pred_text), len(target_text))
                    if min_len > 0:
                        match_chars = sum(1 for a, b in zip(pred_text[:min_len], target_text[:min_len]) if a == b)
                        match_ratio = match_chars / min_len
                        print(f"ðŸ“Š Character match: {match_ratio:.1%} ({match_chars}/{min_len} chars)")
        
        print("="*80 + "\n")